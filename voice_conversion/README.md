# Voice Conversion Using Speech-to-Speech Neuro-Style Transfer

The paper goal its present a new voice conversion method based on a neural style transfer model of the mel-spectrogram.

# Basic information about the project

Main paper / reference: Ehab A. AlBadawy, Siwei Lyu, <"Voice Conversion Using Speech-to-Speech Neuro-Style Transfer">, University at Albany, SUNY, USA

Main dataset: [(https://groups.csail.mit.edu/sls/downloads/flickraudio/downloads.cgi)]

Original code: https://github.com/ebadawy/voice_conversion

slide: https://docs.google.com/presentation/d/1pgDC-qwCtOGZSIG5tjIRUbSUCfo6EeyqRD-j7djDP5E/edit#slide=id.g13331885fbe_0_5

Language: Python 

# Installation

pip install librosa

pip install tqdm

pip install webrtcvad

pip install nnmnkwii

# Executing / performing basic analysis

First of all, a paper was selected from github which it has the codes in python. Downloading files were saved in .zip and uploaded on the server generated on the google cloud platform, where the debian distribution was being simulated.

Use the 'unzip' command to unzip the 'voice_conversion-master' and 'wavenet_vocoder-master'

command: unzip [name].zip

After unzipping the folders with the codes, then the dataset flickr_audio was uploaded to the server

# Credits

14/06/2022 - Tatiane Ferraz Balbinot - your github URL

# References

Main reference: https://github.com/ebadawy/voice_conversion

Auxiliary reference: https://github.com/RussellSB/tt-vae-gan

WaveNet VoCoder source: https://github.com/r9y9/wavenet_vocoder 
